# Optimize

This module contains an interface definition for a simple optimizer and regularizer,
as well as implementations for the most common ones.

## Implemented Optimizers

- [ ] AdaGrad
- [ ] Adam
- [ ] AdamW
- [X] SGD
- [ ] SGDwMomentum

## TODO

- [ ] Add regularization:
    - <https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch>
