# Optimize

This module contains an interface definition for a simple optimizer and regularizer,
as well as implementations for the most common ones.

## Implemented Optimizers

- [ ] AdaGrad
- [ ] Adam
- [ ] AdamW
- [X] SGD
- [ ] SGDwMomentum

## TODO

- [ ] Add regularization:
    - <https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch>
- [ ] Implement learning rate scheduling:
    - <https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863>
